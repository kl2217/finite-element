{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiVi2tB7G4Oqdy10sy+gdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kl2217/finite-element/blob/main/convex_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Vw2lmA8QJequ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimize a convex optimization problem using Newton's method\n",
        "\n",
        "This code demonstrates the application of Newton's method to find the minimum of a convex function. Newton's method is an iterative optimization algorithm that utilizes the function's gradient and Hessian matrix to update its estimate of the minimum.\n",
        "\n",
        "**Mathematical Representation**\n",
        "\n",
        "Let's represent the function, its derivatives, and Newton's method formula using  `ω` (omega) as the variable:\n",
        "\n",
        "**Loss Function (f(ω))**\n",
        "\n",
        "$$\n",
        "f(ω) = ω^2 + 2ω + 1\n",
        "$$\n",
        "\n",
        "**First Derivative (df(ω))**\n",
        "\n",
        "$$\n",
        "df(ω) = \\frac{d}{dω} f(ω) = 2ω + 2\n",
        "$$\n",
        "\n",
        "**Second Derivative (d2f(ω))**\n",
        "\n",
        "$$\n",
        "d2f(ω) = \\frac{d^2}{dω^2} f(ω) = 2\n",
        "$$\n",
        "\n",
        "**Newton's Method Formula**\n",
        "\n",
        "$$\n",
        "ω_{t+1} = ω_t - \\frac{df(ω_t)}{d2f(ω_t)}\n",
        "$$\n",
        "\n",
        "\n",
        "**Convex Optimization with Newton's Method**\n",
        "\n",
        "In convex optimization, Newton's method excels due to its quadratic convergence rate, meaning it can rapidly approach the minimum when starting from a reasonable initial guess. It leverages the curvature information provided by the Hessian matrix to take more informed steps towards the minimum.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Despite its strengths, Newton's method has some limitations:\n",
        "\n",
        "* **Hessian Calculation:** Computing the Hessian matrix can be computationally expensive, especially for high-dimensional problems.\n",
        "* **Non-Convex Functions:** It may not converge or converge to a local minimum for non-convex functions.\n",
        "* **Saddle Points:** It can get stuck at saddle points where the gradient is zero but the Hessian is not positive definite."
      ],
      "metadata": {
        "id": "UxsxaZduJl-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimize a convex optimization problem using Newton's method\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  \"\"\"\n",
        "  Objective function to minimize.\n",
        "  \"\"\"\n",
        "  return x**2 + 2*x + 1\n",
        "\n",
        "def df(x):\n",
        "  \"\"\"\n",
        "  First derivative of the objective function.\n",
        "  \"\"\"\n",
        "  return 2*x + 2\n",
        "\n",
        "def d2f(x):\n",
        "  \"\"\"\n",
        "  Second derivative of the objective function.\n",
        "  \"\"\"\n",
        "  return 2\n",
        "\n",
        "def newtons_method(x0, tolerance=1e-6, max_iterations=100):\n",
        "  \"\"\"\n",
        "  Minimizes a convex function using Newton's method.\n",
        "\n",
        "  Args:\n",
        "    x0: Initial guess for the minimum.\n",
        "    tolerance: Convergence tolerance.\n",
        "    max_iterations: Maximum number of iterations.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing:\n",
        "      - The approximate minimum.\n",
        "      - The number of iterations performed.\n",
        "  \"\"\"\n",
        "  x = x0\n",
        "  for i in range(max_iterations):\n",
        "    delta_x = -df(x) / d2f(x)  # Newton's update rule\n",
        "    x = x + delta_x\n",
        "    if abs(delta_x) < tolerance:\n",
        "      return x, i + 1\n",
        "\n",
        "  return x, max_iterations # Return after max iteration if it doesn't converge\n",
        "\n",
        "# Example usage:\n",
        "initial_guess = 10  # An example starting point\n",
        "minimum, iterations = newtons_method(initial_guess)\n",
        "\n",
        "print(f\"Approximate minimum: {minimum}\")\n",
        "print(f\"Number of iterations: {iterations}\")\n",
        "print(f\"Function value at minimum: {f(minimum)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xV66_g0Jmmc",
        "outputId": "45dd1a3d-8198-41a4-c558-4308b4202993"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate minimum: -1.0\n",
            "Number of iterations: 2\n",
            "Function value at minimum: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In multiple dimensions\n",
        "\n",
        "**Loss Function (f(ω))**\n",
        "\n",
        "$$\n",
        "f(ω) = 100(ω_2 - ω_1^2)^2 + (1 - ω_1)^2\n",
        "$$\n",
        "\n",
        "**Gradient (∇f(ω))**\n",
        "\n",
        "$$\n",
        "∇f(ω) = \\begin{bmatrix}\n",
        "-400ω_1(ω_2 - ω_1^2) - 2(1 - ω_1) \\\\\n",
        "200(ω_2 - ω_1^2)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Hessian (H(ω))**\n",
        "\n",
        "$$\n",
        "H(ω) = \\begin{bmatrix}\n",
        "-400(ω_2 - 3ω_1^2) + 2 & -400ω_1 \\\\\n",
        "-400ω_1 & 200\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Newton's Method Formula (Matrix Form)**\n",
        "\n",
        "$$\n",
        "ω_{t+1} = ω_t - [H(ω_t)]^{-1}∇f(ω_t)\n",
        "$$"
      ],
      "metadata": {
        "id": "lmHvqznXMezA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ ∇f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} $$\n",
        "\n",
        "Explanation:\n",
        "\n",
        "∇f(x): Represents the gradient of the function 'f' at a point 'x'.\n",
        "\n",
        "x₁, x₂, ..., xₙ: Are the input variables of the function 'f'.\n",
        "\n",
        "∂f/∂xᵢ: Represents the first-order partial derivative of the function 'f' with respect to variable 'xᵢ'.\n",
        "\n",
        "The gradient is a vector that points in the direction of the greatest rate of increase of the function. Its components represent the slopes of the function in each direction of the input variables."
      ],
      "metadata": {
        "id": "nFJRBaKlTzLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ H(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} $$\n",
        "\n",
        "Explanation:\n",
        "\n",
        "H(f): Represents the Hessian matrix of a function 'f'.\n",
        "\n",
        "x₁, x₂, ..., xₙ: Are the input variables of the function 'f'.\n",
        "\n",
        "∂²f/∂xᵢ∂xⱼ: Represents the second-order partial derivative of the function 'f' with respect to variable 'xᵢ' and then with respect to variable 'xⱼ'.\n",
        "\n",
        "\n",
        "The Hessian matrix is a square matrix that contains all the second-order partial derivatives of a multivariable function. Each element in the matrix represents how the rate of change of one partial derivative (slope in one direction) is affected by a change in another variable. It essentially captures the curvature of the function's surface."
      ],
      "metadata": {
        "id": "OMKWwINUTOyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Minimize a multi-dimensional convex optimization problem using Newton's method\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Objective function to minimize (example: Rosenbrock function).\n",
        "    \"\"\"\n",
        "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "def df(x):\n",
        "    \"\"\"\n",
        "    Gradient of the objective function.\n",
        "    \"\"\"\n",
        "    grad = np.zeros(2)\n",
        "    grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "    grad[1] = 200 * (x[1] - x[0]**2)\n",
        "    return grad\n",
        "\n",
        "def d2f(x):\n",
        "    \"\"\"\n",
        "    Hessian matrix of the objective function.\n",
        "    \"\"\"\n",
        "    hessian = np.zeros((2, 2))\n",
        "    hessian[0, 0] = -400 * (x[1] - 3 * x[0]**2) + 2\n",
        "    hessian[0, 1] = -400 * x[0]\n",
        "    hessian[1, 0] = -400 * x[0]\n",
        "    hessian[1, 1] = 200\n",
        "    return hessian\n",
        "\n",
        "def newtons_method(x0, tolerance=1e-6, max_iterations=100):\n",
        "    \"\"\"\n",
        "    Minimizes a multi-dimensional convex function using Newton's method.\n",
        "    Imagine you're standing on a hillside and want to find the lowest point (the minimum).\n",
        "    The gradient tells you the direction of the steepest uphill climb.\n",
        "    The Hessian gives you information about the shape of the hill.\n",
        "    This line of code essentially calculates the direction\n",
        "    and distance you should move to go downhill most efficiently,\n",
        "    taking into account the slope and curvature of the hill.\n",
        "    This movement is stored in delta_x. This process is iteratively repeated,\n",
        "    leading us to the valley, i.e., the minimum.\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    for i in range(max_iterations):\n",
        "        # Solve the linear equations: Hessian_matrix * delta_x = -gradient\n",
        "        delta_x = -np.linalg.solve(d2f(x), df(x))\n",
        "        x = x + delta_x\n",
        "        #magnitude or length of the delta_x vector is small enough\n",
        "        if np.linalg.norm(delta_x) < tolerance:\n",
        "            return x, i + 1\n",
        "    return x, max_iterations\n",
        "\n",
        "# Example usage:\n",
        "initial_guess = np.array([-1.2, 1])\n",
        "minimum, iterations = newtons_method(initial_guess)\n",
        "\n",
        "print(f\"Approximate minimum: {minimum}\")\n",
        "print(f\"Number of iterations: {iterations}\")\n",
        "print(f\"Function value at minimum: {f(minimum)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jass2B_zOa30",
        "outputId": "7398c110-9a8f-4143-efc4-0f953488bb41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate minimum: [1. 1.]\n",
            "Number of iterations: 7\n",
            "Function value at minimum: 0.0\n"
          ]
        }
      ]
    }
  ]
}