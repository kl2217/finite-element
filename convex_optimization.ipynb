{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPnLe6S5T/YPVhpTcZed4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kl2217/finite-element/blob/main/convex_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vw2lmA8QJequ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimize a convex optimization problem using Newton's method\n",
        "\n",
        "This code demonstrates the application of Newton's method to find the minimum of a convex function. Newton's method is an iterative optimization algorithm that utilizes the function's gradient and Hessian matrix to update its estimate of the minimum.\n",
        "\n",
        "**Mathematical Representation**\n",
        "\n",
        "Let's represent the function, its derivatives, and Newton's method formula using  `ω` (omega) as the variable:\n",
        "\n",
        "**Loss Function (f(ω))**\n",
        "\n",
        "$$\n",
        "f(ω) = ω^2 + 2ω + 1\n",
        "$$\n",
        "\n",
        "**First Derivative (df(ω))**\n",
        "\n",
        "$$\n",
        "df(ω) = \\frac{d}{dω} f(ω) = 2ω + 2\n",
        "$$\n",
        "\n",
        "**Second Derivative (d2f(ω))**\n",
        "\n",
        "$$\n",
        "d2f(ω) = \\frac{d^2}{dω^2} f(ω) = 2\n",
        "$$\n",
        "\n",
        "**Newton's Method Formula**\n",
        "\n",
        "$$\n",
        "ω_{t+1} = ω_t - \\frac{df(ω_t)}{d2f(ω_t)}\n",
        "$$\n",
        "\n",
        "\n",
        "**Convex Optimization with Newton's Method**\n",
        "\n",
        "In convex optimization, Newton's method excels due to its quadratic convergence rate, meaning it can rapidly approach the minimum when starting from a reasonable initial guess. It leverages the curvature information provided by the Hessian matrix to take more informed steps towards the minimum.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Despite its strengths, Newton's method has some limitations:\n",
        "\n",
        "* **Hessian Calculation:** Computing the Hessian matrix can be computationally expensive, especially for high-dimensional problems.\n",
        "* **Non-Convex Functions:** It may not converge or converge to a local minimum for non-convex functions.\n",
        "* **Saddle Points:** It can get stuck at saddle points where the gradient is zero but the Hessian is not positive definite."
      ],
      "metadata": {
        "id": "UxsxaZduJl-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimize a convex optimization problem using Newton's method\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  \"\"\"\n",
        "  Objective function to minimize.\n",
        "  \"\"\"\n",
        "  return x**2 + 2*x + 1\n",
        "\n",
        "def df(x):\n",
        "  \"\"\"\n",
        "  First derivative of the objective function.\n",
        "  \"\"\"\n",
        "  return 2*x + 2\n",
        "\n",
        "def d2f(x):\n",
        "  \"\"\"\n",
        "  Second derivative of the objective function.\n",
        "  \"\"\"\n",
        "  return 2\n",
        "\n",
        "def newtons_method(x0, tolerance=1e-6, max_iterations=100):\n",
        "  \"\"\"\n",
        "  Minimizes a convex function using Newton's method.\n",
        "\n",
        "  Args:\n",
        "    x0: Initial guess for the minimum.\n",
        "    tolerance: Convergence tolerance.\n",
        "    max_iterations: Maximum number of iterations.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing:\n",
        "      - The approximate minimum.\n",
        "      - The number of iterations performed.\n",
        "  \"\"\"\n",
        "  x = x0\n",
        "  for i in range(max_iterations):\n",
        "    delta_x = -df(x) / d2f(x)  # Newton's update rule\n",
        "    x = x + delta_x\n",
        "    if abs(delta_x) < tolerance:\n",
        "      return x, i + 1\n",
        "\n",
        "  return x, max_iterations # Return after max iteration if it doesn't converge\n",
        "\n",
        "# Example usage:\n",
        "initial_guess = 10  # An example starting point\n",
        "minimum, iterations = newtons_method(initial_guess)\n",
        "\n",
        "print(f\"Approximate minimum: {minimum}\")\n",
        "print(f\"Number of iterations: {iterations}\")\n",
        "print(f\"Function value at minimum: {f(minimum)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xV66_g0Jmmc",
        "outputId": "800fac42-d05b-432b-f852-b6be676688b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate minimum: -1.0\n",
            "Number of iterations: 2\n",
            "Function value at minimum: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In multiple dimensions\n",
        "\n",
        "**Loss Function (f(ω))**\n",
        "\n",
        "$$\n",
        "f(ω) = 100(ω_2 - ω_1^2)^2 + (1 - ω_1)^2\n",
        "$$\n",
        "\n",
        "**Gradient (∇f(ω))**\n",
        "\n",
        "$$\n",
        "∇f(ω) = \\begin{bmatrix}\n",
        "-400ω_1(ω_2 - ω_1^2) - 2(1 - ω_1) \\\\\n",
        "200(ω_2 - ω_1^2)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Hessian (H(ω))**\n",
        "\n",
        "$$\n",
        "H(ω) = \\begin{bmatrix}\n",
        "-400(ω_2 - 3ω_1^2) + 2 & -400ω_1 \\\\\n",
        "-400ω_1 & 200\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Newton's Method Formula (Matrix Form)**\n",
        "\n",
        "$$\n",
        "ω_{t+1} = ω_t - [H(ω_t)]^{-1}∇f(ω_t)\n",
        "$$"
      ],
      "metadata": {
        "id": "lmHvqznXMezA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Minimize a multi-dimensional convex optimization problem using Newton's method\n",
        "\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Objective function to minimize (example: Rosenbrock function).\n",
        "    \"\"\"\n",
        "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "def df(x):\n",
        "    \"\"\"\n",
        "    Gradient of the objective function.\n",
        "    \"\"\"\n",
        "    grad = np.zeros(2)\n",
        "    grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "    grad[1] = 200 * (x[1] - x[0]**2)\n",
        "    return grad\n",
        "\n",
        "def d2f(x):\n",
        "    \"\"\"\n",
        "    Hessian matrix of the objective function.\n",
        "    \"\"\"\n",
        "    hessian = np.zeros((2, 2))\n",
        "    hessian[0, 0] = -400 * (x[1] - 3 * x[0]**2) + 2\n",
        "    hessian[0, 1] = -400 * x[0]\n",
        "    hessian[1, 0] = -400 * x[0]\n",
        "    hessian[1, 1] = 200\n",
        "    return hessian\n",
        "\n",
        "def newtons_method(x0, tolerance=1e-6, max_iterations=100):\n",
        "    \"\"\"\n",
        "    Minimizes a multi-dimensional convex function using Newton's method.\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    for i in range(max_iterations):\n",
        "        delta_x = -np.linalg.solve(d2f(x), df(x))  # Solve the linear system\n",
        "        x = x + delta_x\n",
        "        if np.linalg.norm(delta_x) < tolerance:\n",
        "            return x, i + 1\n",
        "    return x, max_iterations\n",
        "\n",
        "# Example usage:\n",
        "initial_guess = np.array([-1.2, 1])\n",
        "minimum, iterations = newtons_method(initial_guess)\n",
        "\n",
        "print(f\"Approximate minimum: {minimum}\")\n",
        "print(f\"Number of iterations: {iterations}\")\n",
        "print(f\"Function value at minimum: {f(minimum)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jass2B_zOa30",
        "outputId": "86b0fc59-394a-4496-e29c-f88e6c80c30c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate minimum: [1. 1.]\n",
            "Number of iterations: 7\n",
            "Function value at minimum: 0.0\n"
          ]
        }
      ]
    }
  ]
}